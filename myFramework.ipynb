{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab3d00e",
   "metadata": {},
   "source": [
    "# 特徴ベースの知識蒸留フレームワーク\n",
    "\n",
    "## 🧪 仮説\n",
    "生徒モデルの処理の流れを教師モデルに似せることで、生徒モデルは教師モデルの性能に近づくことができる。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 特徴\n",
    "\n",
    "- 中間層の特徴表現を活用\n",
    "- CKA（Centered Kernel Alignment）による層間類似度の評価\n",
    "- 構造的類似性の模倣：処理の流れそのものを模倣する\n",
    "  - 例：教師モデルが序盤で学習する特徴は、生徒モデルも序盤で学習するように誘導する\n",
    "- 層数の違いを吸収可能：層数が異なるモデル間でも蒸留が可能\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 実装手順\n",
    "\n",
    "1. 教師モデルの準備  \n",
    "   - 多くの場合、事前学習済みモデルを使用\n",
    "\n",
    "2. 教師モデルの層間CKA計算  \n",
    "   - 各層の出力に対してCKAを計算し、類似度を評価\n",
    "\n",
    "3. 層のグルーピング  \n",
    "   - 教師モデルの層を CKA に基づいて n グループに分割  \n",
    "   - グループは順番を保持し、隣接する層のみ同じグループに所属可能\n",
    "\n",
    "4. 生徒モデルの分割  \n",
    "   - 生徒モデルも同様に n グループに分割（層数が異なっていても均等に分割）\n",
    "\n",
    "5. グループ対応付け  \n",
    "   - 教師モデルと生徒モデルの各グループを順番に対応させ、n × n の対応関係を構築\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 損失関数設計\n",
    "\n",
    "- 毎バッチで損失を計算\n",
    "- 生徒モデルと教師モデルの各層の出力に対して CKA を計算\n",
    "- L_s × L_t の CKA 類似度マトリクスを構築  \n",
    "  - L_s：生徒モデルの層数  \n",
    "  - L_t：教師モデルの層数\n",
    "\n",
    "- 対角成分に対応する n グループ（G₁, G₂, ..., Gₙ）に注目\n",
    "- 各グループ Gᵢ の代表値（平均など）を算出 → CKA_Gᵢ\n",
    "- 損失として 1 - CKA_Gᵢ を計算\n",
    "- 全グループの損失を統合（加重平均、合計など）して最終損失とする\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ メリット\n",
    "\n",
    "- 層数の違いを吸収しながら、構造的な知識を効果的に蒸留可能\n",
    "- CKAにより、単なる出力の一致ではなく特徴空間の類似性を重視\n",
    "- 処理の流れを模倣することで、より深い知識の転移が可能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb5825",
   "metadata": {},
   "source": [
    "# 実装\n",
    "train_student.pyをベースに実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581116c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import tensorboard_logger as tb_logger\n",
    "# 変更後\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models import model_dict\n",
    "from models.util import ConvReg, SelfA, SRRL, SimKD\n",
    "\n",
    "from dataset.cifar10 import get_cifar10_dataloaders, get_cifar10_dataloaders_sample\n",
    "from dataset.cifar100 import get_cifar100_dataloaders, get_cifar100_dataloaders_sample\n",
    "from dataset.imagenet import get_imagenet_dataloader,  get_dataloader_sample\n",
    "from dataset.cinic10 import get_cinic10_dataloaders, get_cinic10_dataloaders_sample\n",
    "# from dataset.FashionMNIST import get_FashionMNIST_dataloaders, get_FashionMNIST_dataloaders_sample\n",
    "# from dataset.imagenet_dali import get_dali_data_loader\n",
    "\n",
    "from helper.loops import train_distill as train, validate_vanilla, validate_distill\n",
    "from helper.util import save_dict_to_json, reduce_tensor, adjust_learning_rate\n",
    "\n",
    "from crd.criterion import CRDLoss\n",
    "from distiller_zoo import DistillKL, HintLoss, Attention, Similarity, VIDLoss, SemCKDLoss\n",
    "\n",
    "split_symbol = '~' if os.name == 'nt' else ':'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_name(model_path):\n",
    "    \"\"\"parse teacher name\"\"\"\n",
    "    directory = model_path.split('/')[-2]\n",
    "    pattern = ''.join(['S', split_symbol, '(.+)', '_T', split_symbol])\n",
    "    name_match = re.match(pattern, directory)\n",
    "    if name_match:\n",
    "        return name_match[1]\n",
    "    segments = directory.split('_')\n",
    "    if segments[0] == 'wrn':\n",
    "        return segments[0] + '_' + segments[1] + '_' + segments[2]\n",
    "    return segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b48e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher(model_path, n_cls, gpu=None, opt=None):\n",
    "    print('==> loading teacher model')\n",
    "    model_t = get_teacher_name(model_path)\n",
    "    model = model_dict[model_t](num_classes=n_cls)\n",
    "    map_location = None if gpu is None else {'cuda:0': 'cuda:%d' % (gpu if opt.multiprocessing_distributed else 0)}\n",
    "    model.load_state_dict(torch.load(model_path, map_location=map_location)['model'])\n",
    "    print('==> done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_option():\n",
    "\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "    \n",
    "    # basic\n",
    "    parser.add_argument('--print_freq', type=int, default=200, help='print frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=240, help='number of training epochs')\n",
    "    parser.add_argument('--gpu_id', type=str, default='0', help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='150,180,210', help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "\n",
    "    # dataset and model\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100', 'imagenet', 'cinic10'], help='dataset')\n",
    "    parser.add_argument('--model_s', type=str, default='resnet8x4')\n",
    "    parser.add_argument('--path_t', type=str, default=None, help='teacher model snapshot')\n",
    "\n",
    "    # distillation\n",
    "    parser.add_argument('--trial', type=str, default='1', help='trial id')\n",
    "    parser.add_argument('--kd_T', type=float, default=4, help='temperature for KD distillation')\n",
    "    parser.add_argument('--distill', type=str, default='kd', choices=['kd', 'hint', 'attention', 'similarity', 'vid',\n",
    "                                                                      'crd', 'semckd','srrl', 'simkd'])\n",
    "    parser.add_argument('-c', '--cls', type=float, default=1.0, help='weight for classification')\n",
    "    parser.add_argument('-d', '--div', type=float, default=1.0, help='weight balance for KD')\n",
    "    parser.add_argument('-b', '--beta', type=float, default=0.0, help='weight balance for other losses')\n",
    "    parser.add_argument('-f', '--factor', type=int, default=2, help='factor size of SimKD')\n",
    "    parser.add_argument('-s', '--soft', type=float, default=1.0, help='attention scale of SemCKD')\n",
    "\n",
    "    # hint layer\n",
    "    parser.add_argument('--hint_layer', default=1, type=int, choices=[0, 1, 2, 3, 4])\n",
    "\n",
    "    # NCE distillation\n",
    "    parser.add_argument('--feat_dim', default=128, type=int, help='feature dimension')\n",
    "    parser.add_argument('--mode', default='exact', type=str, choices=['exact', 'relax'])\n",
    "    parser.add_argument('--nce_k', default=16384, type=int, help='number of negative samples for NCE')\n",
    "    parser.add_argument('--nce_t', default=0.07, type=float, help='temperature parameter for softmax')\n",
    "    parser.add_argument('--nce_m', default=0.5, type=float, help='momentum for non-parametric updates')\n",
    "\n",
    "    # multiprocessing\n",
    "    parser.add_argument('--dali', type=str, choices=['cpu', 'gpu'], default=None)\n",
    "    parser.add_argument('--multiprocessing-distributed', action='store_true',\n",
    "                    help='Use multi-processing distributed training to launch '\n",
    "                         'N processes per node, which has N GPUs. This is the '\n",
    "                         'fastest way to use PyTorch for either single node or '\n",
    "                         'multi node data parallel training')\n",
    "    parser.add_argument('--dist-url', default='tcp://127.0.0.1:23451', type=str,\n",
    "                    help='url used to set up distributed training')\n",
    "    parser.add_argument('--deterministic', action='store_true', help='Make results reproducible')\n",
    "    parser.add_argument('--skip-validation', action='store_true', help='Skip validation of teacher')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # set different learning rates for these MobileNet/ShuffleNet models\n",
    "    if opt.model_s in ['MobileNetV2', 'MobileNetV2_1_0', 'ShuffleV1', 'ShuffleV2', 'ShuffleV2_1_5']:\n",
    "        opt.learning_rate = 0.01\n",
    "\n",
    "    # set the path of model and tensorboard\n",
    "    opt.model_path = './save/students/models'\n",
    "    opt.tb_path = './save/students/tensorboard'\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_t = get_teacher_name(opt.path_t)\n",
    "\n",
    "    model_name_template = split_symbol.join(['S', '{}_T', '{}_{}_{}_r', '{}_a', '{}_b', '{}_{}'])\n",
    "    opt.model_name = model_name_template.format(opt.model_s, opt.model_t, opt.dataset, opt.distill,\n",
    "                                                opt.cls, opt.div, opt.beta, opt.trial)\n",
    "\n",
    "    if opt.dali is not None:\n",
    "        opt.model_name += '_dali:' + opt.dali\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "    \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c6b51",
   "metadata": {},
   "source": [
    "### 1. 教師モデルの準備\n",
    "既存のモデルを使うこともできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9651bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# cmd = [\n",
    "#     \"python\", \"train_teacher.py\",\n",
    "#     \"--dataset\", \"cinic10\",\n",
    "#     \"--epochs\", \"240\",\n",
    "#     \"--trial\", \"0\",\n",
    "#     \"--model\", \"vgg8\"\n",
    "# ]\n",
    "\n",
    "# subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dc216",
   "metadata": {},
   "source": [
    "### 2. 教師モデルの層間CKA計算  \n",
    "   - 各層の出力に対してCKAを計算し、類似度を評価\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac58b7",
   "metadata": {},
   "source": [
    "### 3. 層のグルーピング  \n",
    "   - 教師モデルの層を CKA に基づいて n グループに分割  \n",
    "   - グループは順番を保持し、隣接する層のみ同じグループに所属可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a8bf7",
   "metadata": {},
   "source": [
    "### 4. 生徒モデルの分割  \n",
    "   - 生徒モデルも同様に n グループに分割（層数が異なっていても均等に分割）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346f8aa",
   "metadata": {},
   "source": [
    "### 5. グループ対応付け  \n",
    "   - 教師モデルと生徒モデルの各グループを順番に対応させ、n × n の対応関係を構築"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
